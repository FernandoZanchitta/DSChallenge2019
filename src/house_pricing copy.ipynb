{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de PO-233 \n",
    "Este projeto busca realizar análise do problema proposto pelo Data Science Challenge 2019 - ITA.\n",
    "\n",
    "##### Alunos:\n",
    "- Fernando Zanchitta\n",
    "- Davi Xie\n",
    "- Hugo Timóteo\n",
    "\n",
    "##### Dados\n",
    "Os dados podem ser adquiridos no site: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.impute import KNNImputer\n",
    "import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/train.csv', index_col = 'Id')\n",
    "test= pd.read_csv('../dataset/test.csv', index_col = 'Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover Dados Faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values:\n",
    "missing_values = pd.DataFrame(data={\n",
    "    'Feature_name': train.columns,\n",
    "    'missing_values': train.isnull().sum(),\n",
    "    'percentage': train.isnull().sum() / len(train) * 100,\n",
    "    'type': train.dtypes\n",
    "})\n",
    "missing_values.sort_values(by='percentage', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remover colunas com mais de 50% de valores faltantes:\n",
    "features_to_drop = missing_values[missing_values['percentage'] > 50]['Feature_name'].values\n",
    "train = train.drop(features_to_drop, axis='columns')\n",
    "test = test.drop(features_to_drop, axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos redundantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contagem de valores únicos:\n",
    "unique_values = pd.DataFrame(data={\"Feature_name\": train.columns, \"unique_values\": train.nunique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values.sort_values(by='unique_values', ascending=True).head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não temos nenhum atributo com valores únicos que possa ser descartado a priori"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção vamos realizar uma análise exploratória dos dados, identificando correlações, distribuições e espalhamento do nosso conjunto de dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para algumas análises vamos trabalhar somente com variáveis numéricas ou categóricas, portanto vamos separar os conjuntos de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "category_features = train.select_dtypes(include=['object']).columns\n",
    "# make list of variables types\n",
    "# we need these lists to indicate Feature-engine which variables it should modify\n",
    "\n",
    "# numerical: discrete\n",
    "discrete = [\n",
    "    var for var in train.columns if train[var].dtype != 'O' and var != 'Survived'\n",
    "    and train[var].nunique() < 10\n",
    "]\n",
    "\n",
    "# numerical: continuous\n",
    "continuous = [\n",
    "    var for var in train.columns\n",
    "    if train[var].dtype != 'O' and var != 'Survived' and var not in discrete\n",
    "]\n",
    "\n",
    "# categorical\n",
    "categorical = [var for var in train.columns if train[var].dtype == 'O' and train[var].nunique() > 1]\n",
    "\n",
    "print('There are {} discrete variables'.format(len(discrete)))\n",
    "print('There are {} continuous variables'.format(len(continuous)))\n",
    "print('There are {} categorical variables'.format(len(categorical)))\n",
    "print(\"numerical_features: \",len(numerical_features))\n",
    "print(\"category_features: \",len(category_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Street\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos verificar a correlação entre atributos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train[numerical_features].corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "plt.title('Matriz de correlação entre as variáveis numéricas.')\n",
    "sns.heatmap(corr_matrix, vmax=0.9, square=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver quais variáveis tem correlação alta com o nosso alvo: _SalesPrice_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlação entre as variáveis numéricas e o preço:\n",
    "corr_matrix['SalePrice'].sort_values(ascending=False).head(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitivamente falando, é provavel que as variáveis mais correlacionadas tenham maior poder preditivo sobre o preço. Entretando devemos verificar se há correlação entre elas mesmas, para fins de simplificação, vamos pegar os atributos cuja correlação seja maior que $0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atributos com correlação maior que 0.5:\n",
    "features_correlated = corr_matrix['SalePrice'].sort_values(ascending=False).loc[lambda x : x > 0.5].index\n",
    "\n",
    "#plotar a correlação entre as  variáveis com correlação maior que 0.5:\n",
    "corr_matrix = train[features_correlated].corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "plt.title('Matriz de correlação entre as variáveis numéricas com correlação maior que 0.5.')\n",
    "sns.heatmap(corr_matrix, vmax=0.9, square=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scatterplot para colunas selecionadas:\n",
    "# sns.set()\n",
    "\n",
    "# sns.pairplot(train[features_correlated], size = 2.5)\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que algumas variáveis são fortemente correlacionadas: \n",
    "-  _GarageArea_ e _GarageCars_ : Faz sentido se pensar que o aumento do numero de carros na garagem exige uma garagem maior.\n",
    "\n",
    "- _TotalBsmtSF_ e _1stFlrSF_: A relação entre o total de metragem do imovel com a metragem do primeiro andar também é pertinente.\n",
    "\n",
    "- _TotRmsAbvGrd_ e _GrLivArea_: O número total de salas em relação ao tamanho da sala de estar também é pertinente\n",
    "\n",
    "\n",
    "Portanto vamos deletar _1stFlrSF_, _GarageArea_ e _GrLivArea_ (Escolhidos de forma arbitrária)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover as variáveis com correlação maior que 0.5:\n",
    "features_correlated = features_correlated.drop(['GarageArea', '1stFlrSF', 'GrLivArea'])\n",
    "train = train.drop(['GarageArea', '1stFlrSF', 'GrLivArea'], axis='columns')\n",
    "test = test.drop(['GarageArea', '1stFlrSF', 'GrLivArea'], axis='columns')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificação vamos escolher somente essas variáveis para nosso modelo:\n",
    "\n",
    "### Todo:\n",
    "- Aumentar a análise exploratoria e entender novas relações entre os dados\n",
    "- Colocar variáveis Categóricas no modelo: verificar criterios, e metodos para isso."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as variáveis para o modelo são as seguintes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_correlated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver a dispersão dos dados em relação a o atributo alvo para cada atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "\n",
    "# Criar um gráfico com 4 linhas e 2 colunas\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 15))\n",
    "\n",
    "# Ajustar o espaçamento entre os subplots\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterar sobre as colunas selecionadas\n",
    "for i, feature in enumerate(features_correlated):\n",
    "    if feature != 'SalePrice':\n",
    "        ax = axes[i]\n",
    "        sns.scatterplot(x=feature, y='SalePrice', data=train, ax=ax)\n",
    "        ax.set_title(f'{feature} vs SalePrice')\n",
    "\n",
    "# remover os subplots vazios:\n",
    "if len(features_correlated) < len(axes):\n",
    "    for j in range(len(features_correlated), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "# ajustar o espaçamento entre os subplots:\n",
    "fig.tight_layout()\n",
    "\n",
    "# Visualizar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engenharia de Atributos\n",
    "Nessa seção vamos realizar insersão de valores, e codificação de atributos\n",
    "## Valores faltantes\n",
    "Vamos verificar as variáveis com valores faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atributos com valores faltantes:\n",
    "missing_values.loc[features_correlated]['missing_values']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não há valores faltantes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Vamos verificar se há valores outliers nos atributos e vamos realizar o tratamento deles\n",
    "### Identificando Outlier com Z- Score\n",
    "O z-score nos da uma idéia do quanto um determinado ponto está afastado da média dos dados, isto é , ele mede quantos desvios padrão abaixo ou acima da média populacional ou amostral os dados estão:\n",
    "$$\n",
    "z=\\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "Onde:\n",
    "- x: observação\n",
    "- $\\mu$: média\n",
    "- $\\sigma$: desvio padrão\n",
    "Assumindo uma distribuição normal, sabe-se que 99,7% dos dados estão à uma distância de três desvios padrão da média. Com base nisso, será considerado nesse trabalho que dados com distância acima de três desvios padrão serão considerados outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar apenas as colunas numéricas do DataFrame train após o drop\n",
    "numerical_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Criar uma cópia do DataFrame train\n",
    "train_ZS = train[numerical_features]\n",
    "\n",
    "# Calcular e atribuir o Z-score para cada coluna numérica, exceto a última\n",
    "for col in numerical_features[:-1]:\n",
    "    col_values = train_ZS[col].values\n",
    "    zscore = stats.zscore(col_values)  # Calcula o Z-score para todos os valores da coluna\n",
    "    outliers = (zscore > 3) | (zscore < -3)\n",
    "    train_ZS.loc[outliers, col] = np.nan\n",
    "\n",
    "# Criação do imputador KNN\n",
    "imputer = KNNImputer(n_neighbors=15, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "# Ajuste do imputador aos dados\n",
    "imputer.fit(train_ZS)\n",
    "\n",
    "# Imputação dos valores ausentes\n",
    "train_ZS = pd.DataFrame(imputer.transform(train_ZS), columns=train_ZS.columns)\n",
    "\n",
    "caminho_arquivo_excel = '../dataset/train_ZS.xlsx'\n",
    "train_ZS.to_excel(caminho_arquivo_excel, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificando Outlier com Amplitude interquartil\n",
    "#### Percentil\n",
    "- percentil 25 : primeiro quaril\n",
    "- percentil 50 : segundo quartil ou mediana\n",
    "- percentil 75 : terceito quartil\n",
    "#### Amplitude interquartil\n",
    "É Diferença entre o terceiro quartil (Q3) e o primeiro quartil (Q1)\n",
    "Para identificar Outlier rom amplitude interquartil serão realizados os procedimentos abaixo:\n",
    "1. Ordenar os dados de forma crescente;\n",
    "2. Calcular o valor do primeiro e terceiro quartil\n",
    "3. Determinar a amplitude interquartil\n",
    "4. Encontrar o limite inferior =  Q1 - 1,5*amplitude interquartil\n",
    "5. Encontrar o limite superior =  Q3 + 1,5*amplitude interquartil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar novo DataFrame com as colunas numéricas\n",
    "train_IQR = train[numerical_features]\n",
    "\n",
    "# Calcular o IQR e identificar outliers\n",
    "for col in numerical_features[:-1]:\n",
    "    Q1 = train_IQR[col].quantile(0.25)\n",
    "    Q3 = train_IQR[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    limIn = Q1 - (IQR * 1.5)\n",
    "    limSp = Q3 + (IQR * 1.5)\n",
    "    \n",
    "    # Substituir outliers por np.nan\n",
    "    train_IQR.loc[(train_IQR[col] < limIn) | (train_IQR[col] > limSp), col] = np.nan\n",
    "\n",
    "# Criar imputador KNN\n",
    "imputer = KNNImputer(n_neighbors=15, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "# Ajustar imputador aos dados\n",
    "imputer.fit(train_IQR)\n",
    "\n",
    "# Imputar valores ausentes\n",
    "train_IQR = pd.DataFrame(imputer.transform(train_IQR), columns=train_IQR.columns)\n",
    "\n",
    "caminho_arquivo_excel = '../dataset/train_IQR.xlsx'\n",
    "train_IQR.to_excel(caminho_arquivo_excel, index=False)\n",
    "\n",
    "caminho_arquivo_excel = '../dataset/train_2.xlsx'\n",
    "train[numerical_features].to_excel(caminho_arquivo_excel, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputando valores aos outliers\n",
    "Em ambos os métodos acima, foram identificados os outliers e para cada um deles foi atribuido um valor ausente, ou seja, eles foram excluidos dos dados.\n",
    "Para preencher os dados ausentes foi utilizada uma técnica de imputação de valores com base em proximidade, denominada KNN. O imputador KNN (K-Nearest Neighbors) é uma técnica dque pode ser utilizada para preencher valores ausentes em conjuntos de dados. Ele é um algoritmo de aprendizado de máquina capaz de prever valores ausentes com base na similaridade entre as amostras do conjunto de dados.\n",
    "\n",
    "Inicialmente ele encontra os K vizinhos mais próximos (específicamente 15 nesse trabalho) para cada valor ausente, calculando a distância entre a amostra com valor ausente e todas as outras amostras no conjunto de dados. Ele seleciona as K amostras mais próximas com base em uma métrica de distância que, neste caso, é a distância euclidiana.\n",
    "\n",
    "Em seguida, o KNN calcula um valor imputado para o valor ausente com base na média arimética dos valores dos vizinhos (vizinhos com pesos iguais). Por fim, o valor imputado é atribuído a cada valor ausente no conjunto de dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção de Atributos (opcional)\n",
    "Nessa seção vamos realizar a seleção de atributos relevantes para o modelo.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino e Teste"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção vamos treinar um modelo de regressão que gere uma predição dos valores de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# regressao linear\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#arvore de deciao\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# SVM\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Adaline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#MLP\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "#cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = train[features_correlated]\n",
    "\n",
    "X = new_train.drop(['SalePrice'], axis=1).copy()\n",
    "Y = new_train['SalePrice'] # here we need to remove unnacessary columns if exist\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos o dataset de treino na proporção de 70-30 em um dataset de treino e outro de teste. Perceba que o tamanho do dataset de teste realmente é 30% do dataset original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_X.shape, train_Y.shape)\n",
    "print(test_X.shape, len(test_Y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro modelo será uma regressão linear utilizando a biblioteca do scikit-learn de linear_model importando a classe LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LR = LinearRegression()\n",
    "LR.fit(train_X, train_Y)\n",
    "\n",
    "LR_predicted = LR.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvore de Decisão"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O segundo modelo será um regressor de Árvore de Decisão utilizando a biblioteca do scikit-learn de tree importando a classe DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTR = DecisionTreeRegressor(criterion='squared_error', max_depth=15, min_samples_split=5, min_samples_leaf=5)\n",
    "DTR.fit(train_X, train_Y)\n",
    "\n",
    "DTR_predicted = DTR.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O terceiro modelo será um SVM utilizando a biblioteca do scikit-learn de svm importando a classe SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVM = make_pipeline(RobustScaler(), RandomForestRegressor())\n",
    "SVM.fit(train_X, train_Y)\n",
    "\n",
    "SVM_predicted = SVM.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor()\n",
    "RF.fit(train_X, train_Y)\n",
    "RF_predicted = RF.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaline - Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ADALINE = SGDRegressor(loss='huber', learning_rate='constant', eta0=0.01, max_iter=1000)\n",
    "ADALINE.fit(train_X, train_Y)\n",
    "\n",
    "ADALINE_predicted = ADALINE.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP - Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPRegressor(hidden_layer_sizes=(40,), activation='relu', solver='adam', max_iter=1000, early_stopping=True, validation_fraction=0.1)\n",
    "\n",
    "MLP.fit(train_X, train_Y)\n",
    "MLP_predicted = MLP.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NB = GaussianNB(var_smoothing=1e-9)\n",
    "\n",
    "NB.fit(train_X, train_Y)\n",
    "NB_predicted = NB.predict(test_X)\n",
    "proba = NB.predict_proba(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção vamos avaliar a performance do modelo gerado.\n",
    "\n",
    "Os principais critério que vamos avaliar vão ser os seguintes:\n",
    "- $R^2$:\n",
    "O primeiro é o coeficiente de determinação, usualmente expresso por $R^2$. O coeficiente de determinação é a razão da variância do alvo, explicado ou predito pelo modelo, pela a variância total do alvo. É um valor com limites entre $0$ e $1$, e quanto mais próximo de 1 maior a capacidade do modelo em explicar ou prever a variância da variável alvo.\n",
    "$$R^2 = \\frac{\\sum(\\hat{y} - \\bar{y})^2}{\\sum(y - \\bar{y})^2}$$\n",
    "\n",
    "- $RMSE$:\n",
    "A segunda métrica é o erro médio quadrático (_Mean Squared Error_). O $MSE$ é a média das diferenças entre o valor alvo predito e o valor real ao quadrado. Nesse sentido, é sempre maior que zero, e quanto menor o valor de $MSE$ maior a acurácia das predições do modelo. Nesse projeto, iremos utilizar a raiz quadrada de $MSE$, chamada de $RMSE$.\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}(y_i-p_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# avaliando\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from functions.actual_vs_pred_plot import actual_vs_pred_plot\n",
    "from functions.model_residual_plot import model_residual_plot\n",
    "from functions.regression_metrics import regression_metrics\n",
    "from functions.model_dist_plot import model_dist_plot\n",
    "import seaborn as sns\n",
    "\n",
    "list_models_name = [\n",
    "    \"Regressão Linear\",\n",
    "    \"Árvore de Decisão\",\n",
    "    \"SVM\",\n",
    "    \"Random Forest\",\n",
    "    \"ADALINE\",\n",
    "    \"MLP\",\n",
    "    \"Naive Bayes\",\n",
    "]\n",
    "list_models = [LR, DTR, SVM,RF,ADALINE, MLP, NB]\n",
    "list_models_predicted = [LR_predicted, DTR_predicted, SVM_predicted, RF_predicted,ADALINE_predicted, MLP_predicted, NB_predicted]\n",
    "list_models_RMSE = []\n",
    "list_models_R2 = []\n",
    "\n",
    "for i in range(0, len(list_models)):\n",
    "    model_predicted = list_models_predicted[i]\n",
    "    model_name = list_models_name[i]\n",
    "    print(\"\\nModelo \" + model_name + \":\")\n",
    "\n",
    "    rmse, r2 = regression_metrics(model_predicted, test_Y,model_name)\n",
    "    list_models_RMSE.append(rmse)\n",
    "    list_models_R2.append(r2)\n",
    "\n",
    "    actual_vs_pred_plot(test_Y, model_predicted,model_name)\n",
    "\n",
    "    # Create residual plot\n",
    "    model_residual_plot(test_Y, model_predicted, model_name)\n",
    "    model_dist_plot(test_Y, model_predicted, model_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando os $RMSE$ de cada modelo gerado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparando os modelos por RMSE:\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.barplot(x=list_models_name, y=list_models_RMSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, seguindo a métrica de _Root Mean Squared Error_ como a principal para fazer seleção dos modelos, o melhor modelo foi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = np.argmin(list_models_RMSE)\n",
    "model_name = list_models_name[model_index]\n",
    "model_predicted = list_models_predicted[model_index]\n",
    "print(\"\\nModelo com melhor R2: \" + model_name)\n",
    "print(\"RMSE: \" + str(list_models_RMSE[model_index]))\n",
    "print(\"R2: \" + str(list_models_R2[model_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "melhor_modelo = SVM\n",
    "\n",
    "validate_columns = features_correlated.drop(['SalePrice'])\n",
    "validate_df = test[validate_columns]\n",
    "\n",
    "# Eliminate these lines after - Preprocess the data to handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "validate_df = pd.DataFrame(imputer.fit_transform(validate_df), columns=validate_columns)\n",
    "\n",
    "validate = validate_df.copy()\n",
    "validate['SalePrice'] = melhor_modelo.predict(validate_df)\n",
    "\n",
    "validate\n",
    "\n",
    "predicted_sale_price = validate\n",
    "predicted_sale_price.to_csv(\"SalePrice_predicted.csv\", index=False)\n",
    "predicted_sale_price.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando Pipelines e Validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline([\n",
    "    (imputer)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerações finais"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
