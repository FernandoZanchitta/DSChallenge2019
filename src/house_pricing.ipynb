{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de PO-233 \n",
    "Este projeto busca realizar análise do problema proposto pelo Data Science Challenge 2019 - ITA.\n",
    "\n",
    "##### Alunos:\n",
    "- Fernando Zanchitta\n",
    "- Davi Xie\n",
    "- Hugo Timóteo\n",
    "\n",
    "##### Dados\n",
    "Os dados podem ser adquiridos no site: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "O campo do aprendizado de máquina e da ciência de dados tem se mostrado cada vez mais relevante e aplicável em diversas áreas. Entre as áreas de grande interesse encontra-se a precificação de imóveis residenciais, um desafio que envolve a análise de várias características do imóvel e a estimativa do seu valor de mercado. Essa é uma tarefa complexa, devido à grande quantidade de fatores que influenciam o problema. Características como tamanho, localização, qualidade, idade, entre outras, desempenham um papel fundamental na determinação do preço de venda. No entanto, a relação entre essas características e o preço não é linear e muitas vezes não é facilmente mensurável. Além disso, existem fatores externos e tendências de mercado que também impactam o valor dos imóveis. Portanto, é um desafio desenvolver modelos que sejam capazes de estimar com precisão os preços de venda.\n",
    "\n",
    "Com base nisso, objetivo deste trabalho é explorar o uso de técnicas de aprendizado indutivo, análise exploratória de dados e aprendizado preditivo para construir um modelo de regressão capaz de estimar o preço de venda de imóveis residenciais. Para isso, utilizaremos o conjunto de dados \"House Prices - Advanced Regression Techniques\", que contém informações detalhadas sobre características dos imóveis, como tamanho, localização, qualidade, idade, entre outras.\n",
    "\n",
    "A fim de se alcançar o objetivo proposto, inicialmente será realizda uma análise exploratória dos dados, aplicando estatísticas descritivas e visualização multivariada para compreender a distribuição e relação entre as variáveis do conjunto de dados. Em seguida, faremos o pré-processamento dos dados, incluindo limpeza, redução dimensional e transformações, a fim de preparar os dados para a construção do modelo de regressão. Em sequência, serão utilizadas técnicas de aprendizado preditivo para treinar e avaliar diferentes modelos de regressão, utilizando métricas de desempenho adequadas para a tarefa de precificação de imóveis. Por fim, serão avaliadas a capacidade de generalização dos modelos por meio de validação cruzada e análise das métricas de erro.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrição da base de dados\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.impute import KNNImputer\n",
    "import functions\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando a leitura do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/train.csv', index_col = 'Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remover Dados Faltantes:\n",
    "Existem técnicas de inserção de variáveis que irão ser mostradas posteriormente. Entretanto, para alguns tipos de atributos cujo percentual de variáveis faltando é muito alto, fazer o tratamento dos valores faltantes gera muito ruído para predição. Portanto, decidiu-se retirar os atributos com percentual de valores faltantes maior que $50\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values:\n",
    "\n",
    "missing_values = pd.DataFrame(data={\n",
    "    'Feature_name': train.columns,\n",
    "    'missing_values': train.isnull().sum(),\n",
    "    'percentage': train.isnull().sum() / len(train) * 100,\n",
    "    'type': train.dtypes\n",
    "})\n",
    "missing_values.sort_values(by='percentage', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remover colunas com mais de 50% de valores faltantes:\n",
    "features_to_drop = missing_values[missing_values['percentage'] > 50]['Feature_name'].values\n",
    "train = train.drop(features_to_drop, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atributos redundantes\n",
    "Nessa seção, investigaremos se há algum atributo com um único valor. Esses atributos não possuem poder preditivo sobre o modelo, e poderiam ser descartados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contagem de valores únicos:\n",
    "unique_values = pd.DataFrame(data={\"Feature_name\": train.columns, \"unique_values\": train.nunique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values.sort_values(by='unique_values', ascending=True).head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não temos nenhum atributo com valores únicos que possa ser descartado a priori"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Exploratória"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise exploratória de dados é um processo inicial na análise de dados, no qual o objetivo é obter uma compreensão básica dos dados e identificar padrões e tendências. É uma etapa crucial antes de aplicar técnicas estatísticas mais avançadas ou construir modelos preditivos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise Exploratória para Dados Quantitativos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determina os atributos numéricos\n",
    "numerical_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Determina os atributos quantitativos discretos e continuos\n",
    "\n",
    "\n",
    "discreet_quantitative = []\n",
    "continuous_quantitative = []\n",
    "for column in train[numerical_features].columns:\n",
    "    unique_values = train[numerical_features][column].nunique()\n",
    "    if unique_values <= 16:\n",
    "        discreet_quantitative.append(column)\n",
    "    else:\n",
    "        continuous_quantitative.append(column)    \n",
    "\n",
    "# Exibe as colunas selecionadas\n",
    "print(discreet_quantitative)\n",
    "print(continuous_quantitative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"quantidade de atributos quantitativos discretos: \", len(discreet_quantitative))\n",
    "print(\"quantidade de atributos quantitativos continuos: \", len(continuous_quantitative))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumo estatístico: \n",
    "\n",
    "Calculam-se medidas estatísticas como média, mediana, moda, desvio padrão, mínimo, máximo e quartis. Essas medidas fornecem uma visão geral das características centrais, dispersão e distribuição dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula o resumo estatístico usando a função describe()\n",
    "summary = train[continuous_quantitative].describe().transpose()\n",
    "    \n",
    "# Adiciona a moda como uma nova coluna no resumo estatístico\n",
    "summary['moda'] = train[continuous_quantitative].mode().transpose()[0]\n",
    "    \n",
    "# Define os nomes das colunas da tabela\n",
    "colunas = ['Atributo', 'Média', 'Mediana', 'Moda', 'Desvio Padrão', 'Mínimo', '25%', '50%', '75%', 'Máximo']\n",
    "    \n",
    "# Cria a matriz com o resumo estatístico\n",
    "matriz_resumo = pd.DataFrame(columns=colunas)\n",
    "matriz_resumo['Atributo'] = summary.index\n",
    "matriz_resumo[['Média', 'Mediana', 'Moda', 'Desvio Padrão', 'Mínimo', '25%', '50%', '75%', 'Máximo']] = summary[['mean', '50%', 'moda', 'std', 'min', '25%', '50%', '75%', 'max']].values\n",
    "\n",
    "# Imprime a tabela\n",
    "display(matriz_resumo)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização de dados:\n",
    "Utilizam-se gráficos e visualizações adequados para representar os dados quantitativos. Isso pode incluir histogramas, box plots, gráficos de dispersão, gráficos de linha ou gráficos de séries temporais, dependendo da natureza dos dados e do objetivo da análise.\n",
    "- Gráficos de dispersão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o estilo do Seaborn\n",
    "sns.set()\n",
    "\n",
    "# Definir o número de subplots por linha e o tamanho da figura\n",
    "subplots_per_row = 3\n",
    "figsize = (15, 15)\n",
    "\n",
    "# Obter todas as colunas de atributos do dataframe train (exceto a última coluna)\n",
    "attribute_columns = train[continuous_quantitative].columns[:-1]\n",
    "\n",
    "# Calcular o número total de subplots\n",
    "num_subplots = len(attribute_columns)\n",
    "\n",
    "# Calcular o número de linhas\n",
    "num_rows = (num_subplots - 1) // subplots_per_row + 1\n",
    "\n",
    "# Criar a figura e os subplots\n",
    "fig, axes = plt.subplots(num_rows, subplots_per_row, figsize=figsize)\n",
    "\n",
    "# Converter a matriz de subplots em um array unidimensional\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterar sobre os atributos e criar os gráficos de dispersão\n",
    "for i, attribute in enumerate(attribute_columns):\n",
    "    # Selecionar o subplot atual\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Criar o gráfico de dispersão\n",
    "    sns.scatterplot(x=attribute, y=train.columns[-1], data=train, ax=ax)\n",
    "    \n",
    "    # Definir o título do gráfico\n",
    "    ax.set_title(f'{attribute} vs SalePrice')\n",
    "    \n",
    "    # Definir o nome do eixo y como o nome do atributo alvo\n",
    "    ax.set_ylabel(train[continuous_quantitative].columns[-1])\n",
    "    \n",
    "# Remover os subplots vazios, se houverem\n",
    "if num_subplots < len(axes):\n",
    "    for j in range(num_subplots, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "# Ajustar o espaçamento entre os subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o estilo do Seaborn\n",
    "sns.set()\n",
    "\n",
    "# Definir o número de subplots por linha e o tamanho da figura\n",
    "subplots_per_row = 3\n",
    "figsize = (15, 15)\n",
    "\n",
    "# Obter todas as colunas de atributos do dataframe train (exceto a última coluna)\n",
    "attribute_columns = train[continuous_quantitative].columns[:-1]\n",
    "\n",
    "# Calcular o número total de subplots\n",
    "num_subplots = len(attribute_columns)\n",
    "\n",
    "# Calcular o número de linhas\n",
    "num_rows = (num_subplots - 1) // subplots_per_row + 1\n",
    "\n",
    "# Criar a figura e os subplots\n",
    "fig, axes = plt.subplots(num_rows, subplots_per_row, figsize=figsize)\n",
    "\n",
    "# Converter a matriz de subplots em um array unidimensional\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterar sobre os atributos e criar os histogramas com as curvas de distribuição\n",
    "for i, attribute in enumerate(attribute_columns):\n",
    "    # Selecionar o subplot atual\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Criar o histograma com a curva de distribuição\n",
    "    sns.histplot(data=train, x=attribute, kde=True, ax=ax)\n",
    "    \n",
    "    # Definir o título do subplot\n",
    "    ax.set_title(attribute)\n",
    "    \n",
    "# Remover os subplots vazios, se houverem\n",
    "if num_subplots < len(axes):\n",
    "    for j in range(num_subplots, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "# Ajustar o espaçamento entre os subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de correlação:\n",
    "Explora-se a relação entre diferentes variáveis quantitativas por meio de medidas de correlação, como o coeficiente de correlação de Pearson. Isso ajuda a identificar a força e a direção da relação entre as variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train[continuous_quantitative].corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "plt.title('Matriz de correlação entre as variáveis numéricas.')\n",
    "sns.heatmap(corr_matrix, vmax=0.9, square=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver quais variáveis tem correlação alta com o nosso alvo: _SalesPrice_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlação entre as variáveis numéricas e o preço:\n",
    "corr_matrix['SalePrice'].sort_values(ascending=False).head(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitivamente falando, é provavel que as variáveis mais correlacionadas tenham maior poder preditivo sobre o preço. Entretando devemos verificar se há correlação entre elas mesmas, para fins de simplificação, vamos pegar os atributos cuja correlação seja maior que $0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atributos com correlação maior que 0.5:\n",
    "features_correlated = corr_matrix['SalePrice'].sort_values(ascending=False).loc[lambda x : x > 0.5].index\n",
    "\n",
    "#plotar a correlação entre as  variáveis com correlação maior que 0.5:\n",
    "corr_matrix = train[features_correlated].corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "plt.title('Matriz de correlação entre as variáveis numéricas com correlação maior que 0.5.')\n",
    "sns.heatmap(corr_matrix, vmax=0.9, square=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que algumas variáveis são fortemente correlacionadas: \n",
    "-  _GarageArea_ e _GarageCars_ : Faz sentido se pensar que o aumento do numero de carros na garagem exige uma garagem maior.\n",
    "\n",
    "- _TotalBsmtSF_ e _1stFlrSF_: A relação entre o total de metragem do imovel com a metragem do primeiro andar também é pertinente.\n",
    "\n",
    "- _TotRmsAbvGrd_ e _GrLivArea_: O número total de salas em relação ao tamanho da sala de estar também é pertinente\n",
    "\n",
    "\n",
    "Portanto vamos deletar _1stFlrSF_, _GarageArea_ e _GrLivArea_ (Escolhidos de forma arbitrária)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover as variáveis com correlação maior que 0.5:\n",
    "features_correlated = features_correlated.drop(['GarageArea', '1stFlrSF', 'GrLivArea'])\n",
    "train = train.drop(['GarageArea', '1stFlrSF', 'GrLivArea'], axis='columns')\n",
    "# test = test.drop(['GarageArea', '1stFlrSF', 'GrLivArea'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determina novamente os atributos numéricos após a exclusão de dados\n",
    "numerical_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Determina os atributos quantitativos discretos e continuos\n",
    "\n",
    "discreet_quantitative = []\n",
    "continuous_quantitative = []\n",
    "for column in train[numerical_features].columns:\n",
    "    unique_values = train[numerical_features][column].nunique()\n",
    "    if unique_values <= 16:\n",
    "        discreet_quantitative.append(column)\n",
    "    else:\n",
    "        continuous_quantitative.append(column)    \n",
    "\n",
    "# Exibe as colunas selecionadas\n",
    "print(discreet_quantitative)\n",
    "print(\"Quantidade de atributos discretos: \", len(discreet_quantitative))\n",
    "print(continuous_quantitative)\n",
    "print(\"Quantidade de atributos contínuos: \", len(continuous_quantitative))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identificação de outliers: \n",
    "\n",
    "Vamos verificar se há valores outliers nos atributos e vamos realizar o tratamento deles. Inicialmente iremos fazer uma inspeção visual por meio do plot de boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o estilo do Seaborn\n",
    "sns.set()\n",
    "\n",
    "# Configurar o layout dos subplots\n",
    "n = 4 # Número de subplots por linha\n",
    "num_features = len(continuous_quantitative)\n",
    "num_subplots = num_features // n + (num_features % n > 0)\n",
    "fig, axes = plt.subplots(num_subplots, 4, figsize=(15, num_subplots * 5))\n",
    "\n",
    "# Iterar sobre as features numéricas (exceto a coluna do atributo alvo)\n",
    "for i, feature in enumerate(continuous_quantitative):\n",
    "    ax = axes[i // n, i % n]  # Acessar o subplot correspondente\n",
    "    sns.boxplot(y=train[feature], ax=ax, orient='v')  # Plotar boxplot na vertical\n",
    "    ax.set_title(feature)  # Definir o título do subplot\n",
    "\n",
    "# Remover subplots vazios, se necessário\n",
    "if num_features % n != 0:\n",
    "    for i in range(num_features % n, n):\n",
    "        fig.delaxes(axes[-1, i])\n",
    "\n",
    "# Ajustar o espaçamento entre os subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identificando Outlier com Z- Score\n",
    "O z-score nos da uma idéia do quanto um determinado ponto está afastado da média dos dados, isto é , ele mede quantos desvios padrão abaixo ou acima da média populacional ou amostral os dados estão:\n",
    "$$\n",
    "z=\\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "Onde:\n",
    "- x: observação\n",
    "- $\\mu$: média\n",
    "- $\\sigma$: desvio padrão\n",
    "Assumindo uma distribuição normal, sabe-se que 99,7% dos dados estão à uma distância de três desvios padrão da média. Com base nisso, será considerado nesse trabalho que dados com distância acima de três desvios padrão serão considerados outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filtrar apenas as colunas numéricas do DataFrame train após o drop\n",
    "# numerical_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# # Criar uma cópia do DataFrame train\n",
    "# train_ZS = train[continuous_quantitative]\n",
    "\n",
    "# # Calcular e atribuir o Z-score para cada coluna numérica, exceto a última\n",
    "# for col in continuous_quantitative[:-1]:\n",
    "#     col_values = train_ZS[col].values\n",
    "#     zscore = stats.zscore(col_values)  # Calcula o Z-score para todos os valores da coluna\n",
    "#     outliers = (zscore > 3) | (zscore < -3)\n",
    "#     train_ZS.loc[outliers, col] = np.nan\n",
    "\n",
    "# # Criação do imputador KNN\n",
    "# imputer = KNNImputer(n_neighbors=15, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "# # Ajuste do imputador aos dados\n",
    "# imputer.fit(train_ZS)\n",
    "\n",
    "# # Imputação dos valores ausentes\n",
    "# train_ZS = pd.DataFrame(imputer.transform(train_ZS), columns=train_ZS.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identificando Outlier com Amplitude interquartil\n",
    "\n",
    "Percentil:\n",
    "\n",
    "- percentil 25 : primeiro quaril\n",
    "- percentil 50 : segundo quartil ou mediana\n",
    "- percentil 75 : terceito quartil\n",
    "\n",
    "Amplitude interquartil:\n",
    "\n",
    "É Diferença entre o terceiro quartil (Q3) e o primeiro quartil (Q1)\n",
    "Para identificar Outlier rom amplitude interquartil serão realizados os procedimentos abaixo:\n",
    "1. Ordenar os dados de forma crescente;\n",
    "2. Calcular o valor do primeiro e terceiro quartil\n",
    "3. Determinar a amplitude interquartil\n",
    "4. Encontrar o limite inferior =  Q1 - 1,5*amplitude interquartil\n",
    "5. Encontrar o limite superior =  Q3 + 1,5*amplitude interquartil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Criar novo DataFrame com as colunas numéricas\n",
    "# train_IQR = train[continuous_quantitative]\n",
    "\n",
    "# # Calcular o IQR e identificar outliers\n",
    "# for col in continuous_quantitative[:-1]:\n",
    "#     Q1 = train_IQR[col].quantile(0.25)\n",
    "#     Q3 = train_IQR[col].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "    \n",
    "#     limIn = Q1 - (IQR * 1.5)\n",
    "#     limSp = Q3 + (IQR * 1.5)\n",
    "    \n",
    "#     # Substituir outliers por np.nan\n",
    "#     train_IQR.loc[(train_IQR[col] < limIn) | (train_IQR[col] > limSp), col] = np.nan\n",
    "\n",
    "# # Criar imputador KNN\n",
    "# imputer = KNNImputer(n_neighbors=15, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "# # Ajustar imputador aos dados\n",
    "# imputer.fit(train_IQR)\n",
    "\n",
    "# # Imputar valores ausentes\n",
    "# train_IQR = pd.DataFrame(imputer.transform(train_IQR), columns=train_IQR.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputando valores aos outliers\n",
    "Em ambos os métodos acima, foram identificados os outliers e para cada um deles foi atribuido um valor ausente, ou seja, eles foram excluidos dos dados.\n",
    "Para preencher os dados ausentes foi utilizada uma técnica de imputação de valores com base em proximidade, denominada KNN. O imputador KNN (K-Nearest Neighbors) é uma técnica dque pode ser utilizada para preencher valores ausentes em conjuntos de dados. Ele é um algoritmo de aprendizado de máquina capaz de prever valores ausentes com base na similaridade entre as amostras do conjunto de dados.\n",
    "\n",
    "Inicialmente ele encontra os K vizinhos mais próximos (específicamente 15 nesse trabalho) para cada valor ausente, calculando a distância entre a amostra com valor ausente e todas as outras amostras no conjunto de dados. Ele seleciona as K amostras mais próximas com base em uma métrica de distância que, neste caso, é a distância euclidiana.\n",
    "\n",
    "Em seguida, o KNN calcula um valor imputado para o valor ausente com base na média arimética dos valores dos vizinhos (vizinhos com pesos iguais). Por fim, o valor imputado é atribuído a cada valor ausente no conjunto de dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise Exploratória para Dados Qualitativos:\n",
    "Inicialmente vamos converter os atributos do tipo 'object' para 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar as colunas do tipo 'object'\n",
    "object_columns = train.select_dtypes(include='object').columns\n",
    "\n",
    "# Converter as colunas para o tipo 'category'\n",
    "train[object_columns] = train[object_columns].astype('category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após isso, vamos selecionar os dados qualitativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical = train.columns[~train.columns.isin(train[continuous_quantitative].columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical: discrete\n",
    "discrete = [\n",
    "    var for var in train.columns if train[var].dtype != 'category' and var != 'SalePrice'\n",
    "    and train[var].nunique() < 10\n",
    "]\n",
    "\n",
    "# numerical: continuous\n",
    "continuous = [\n",
    "    var for var in train.columns\n",
    "    if train[var].dtype != 'category' and var != 'SalePrice' and var not in discrete\n",
    "]\n",
    "\n",
    "# categorical\n",
    "categorical = [var for var in train.columns if train[var].dtype == 'category' and train[var].nunique() > 1]\n",
    "\n",
    "print('There are {} discrete variables'.format(len(discrete)))\n",
    "print('There are {} continuous variables'.format(len(continuous)))\n",
    "print('There are {} categorical variables'.format(len(categorical)))\n",
    "# print(\"numerical_features: \",len(numerical_features))\n",
    "# print(\"category_features: \",len(category_features))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para dados qualitativos, que envolvem atributos ou categorias, a análise exploratória pode envolver os seguintes aspectos:\n",
    "\n",
    "#### Frequência e distribuição: \n",
    "Calculam-se as frequências absolutas e relativas de cada categoria presente nos dados qualitativos. Isso permite entender a distribuição e proporções de cada categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os atributos em duas metades\n",
    "half = len(train[categorical]) // 2\n",
    "first_half = train[categorical][:half]\n",
    "second_half = train[categorical][half:]\n",
    "\n",
    "# Configurações do estilo seaborn\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Configuração dos subplots para a primeira metade dos atributos\n",
    "num_first_half = len(first_half.columns)\n",
    "num_rows_first_half = (num_first_half - 1) // 4 + 1\n",
    "fig, axes = plt.subplots(nrows=num_rows_first_half, ncols=4, figsize=(15, 4*num_rows_first_half))\n",
    "\n",
    "# Plotagem dos histogramas para a primeira metade dos atributos\n",
    "for i, col in enumerate(first_half.columns):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    sns.histplot(data=train, x=col, ax=ax, stat='count', discrete=True)\n",
    "    ax.set_title(col, fontsize=10)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    # Definir as etiquetas do eixo x na vertical\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "    \n",
    "\n",
    "# Remover subplots vazios, se houverem\n",
    "if num_first_half < num_rows_first_half * 4:\n",
    "    for i in range(num_first_half, num_rows_first_half * 4):\n",
    "        fig.delaxes(axes[i // 4, i % 4])\n",
    "\n",
    "# Ajuste de espaçamento entre subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Configuração dos subplots para a segunda metade dos atributos\n",
    "num_second_half = len(second_half.columns)\n",
    "num_rows_second_half = (num_second_half - 1) // 4 + 1\n",
    "fig, axes = plt.subplots(nrows=num_rows_second_half, ncols=4, figsize=(15, 4*num_rows_second_half))\n",
    "\n",
    "# Plotagem dos histogramas para a segunda metade dos atributos\n",
    "for i, col in enumerate(second_half.columns):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    sns.histplot(data=train, x=col, ax=ax, stat='count', discrete=True)\n",
    "    ax.set_title(col, fontsize=10)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    # Definir as etiquetas do eixo x na vertical\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "    \n",
    "\n",
    "# Remover subplots vazios, se houverem\n",
    "if num_second_half < num_rows_second_half * 4:\n",
    "    for i in range(num_second_half, num_rows_second_half * 4):\n",
    "        fig.delaxes(axes[i // 4, i % 4])\n",
    "\n",
    "# Ajuste de espaçamento entre subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibição dos plots\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribuição do atributo alvo em relação aos demais atributos\n",
    "\n",
    "A distribuição do atributo alvo em relação aos demais atributos refere-se à relação entre a variável alvo (também conhecida como variável dependente, resposta ou variável a ser prevista) e as outras variáveis do conjunto de dados (também conhecidas como variáveis independentes, preditoras ou explicativas).\n",
    "\n",
    "Essa análise busca compreender como a distribuição ou os valores da variável alvo variam ou são influenciados pelas diferentes categorias ou níveis das outras variáveis. Em outras palavras, examina-se como a variável alvo se comporta em relação a diferentes valores ou grupos das outras variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter atributos qualitativos para tipo categórico\n",
    "# for col in categorical:\n",
    "#     train[col] = train[col].astype('category')\n",
    "\n",
    "    # Verificar valores ausentes\n",
    "    # if train[col].isnull().any():\n",
    "        # train[col] = train[col].cat.add_categories(['MISSING'])\n",
    "        # train[col] = train[col].fillna('MISSING')\n",
    "\n",
    "def boxplot(x, y, **kwargs):\n",
    "    sns.boxplot(x=x, y=y)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "# Derreter o DataFrame\n",
    "melted = pd.melt(train, id_vars=['SalePrice'], value_vars=categorical)\n",
    "\n",
    "# Configurar o FacetGrid para os boxplots\n",
    "g = sns.FacetGrid(melted, col=\"variable\", col_wrap=4, sharex=False, sharey=False, height=5)\n",
    "\n",
    "# Mapear a função boxplot no FacetGrid\n",
    "g.map(boxplot, \"value\", \"SalePrice\")\n",
    "\n",
    "# Exibir os plots\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de variância (ANOVA) \n",
    "\n",
    "A ANOVA é um teste estatístico utilizado para comparar a média de um grupo com a média de outros grupos, permitindo determinar se existem diferenças significativas entre os grupos. No gráfico do ANOVA, a disparidade (disparity) é representada no eixo y em uma escala logarítmica. Valores mais altos indicam uma maior disparidade entre as categorias da variável qualitativa em relação à variável de resposta (SalePrice no caso desse código).\n",
    "\n",
    "Em termos práticos, uma disparidade alta indica que as categorias da variável qualitativa têm uma influência significativa na variável de resposta e podem ser consideradas importantes para explicar as variações nos valores da variável de resposta. Isso sugere que a variável qualitativa pode ser um bom preditor da variável de resposta.\n",
    "\n",
    "Por outro lado, uma disparidade baixa indica que as categorias da variável qualitativa têm pouca influência ou não são estatisticamente significativas na explicação das variações na variável de resposta. Isso sugere que a variável qualitativa pode não ser relevante ou informativa para prever a variável de resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anv = pd.DataFrame()\n",
    "anv['feature'] = train[categorical].columns\n",
    "pvals = []\n",
    "for c in train[categorical].columns:\n",
    "    samples = []\n",
    "    for cls in train[train[categorical][c].notnull()][c].unique():\n",
    "        s = train[train[categorical][c] == cls]['SalePrice'].values\n",
    "        samples.append(s)\n",
    "    pval = stats.f_oneway(*samples)[1]\n",
    "    pvals.append(pval)\n",
    "anv['pval'] = pvals\n",
    "anv['disparity'] = np.log(1. / np.maximum(anv['pval'].values, 1e-100))\n",
    "anv = anv.sort_values('disparity', ascending=False)\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(data=anv, x='feature', y='disparity')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Disparity (log-scale)')\n",
    "plt.title('ANOVA - Disparity of Qualitative Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Teste qui-quadrado:\n",
    "O teste qui-quadrado, também conhecido como teste de qui-quadrado de Pearson, é um teste estatístico utilizado para determinar se existe uma associação significativa entre duas variáveis categóricas. Ele baseia-se na comparação entre as frequências observadas e as frequências esperadas sob a hipótese nula de que não há associação entre as variáveis. A ideia é verificar se as diferenças observadas entre as frequências são grandes o suficiente para serem consideradas estatisticamente significativas.\n",
    "\n",
    "O valor de p-value é uma medida estatística que indica a evidência contra a hipótese nula em um teste estatístico. Em geral, ele é usado para avaliar se existe uma associação significativa entre duas variáveis em um teste de independência, como o teste do qui-quadrado.\n",
    "\n",
    "- Um p-value baixo (geralmente menor que 0,05 ou 0,01) indica que há evidências estatísticas significativas para rejeitar a hipótese nula. Isso sugere que existe uma associação ou diferença estatisticamente significativa entre as variáveis testadas.\n",
    "\n",
    "- Um p-value alto (geralmente maior que 0,05 ou 0,01) indica que não há evidências suficientes para rejeitar a hipótese nula. Isso sugere que não há uma associação ou diferença estatisticamente significativa entre as variáveis testadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar o teste qui-quadrado e obter os resultados\n",
    "results = []\n",
    "for column in train[categorical].columns:\n",
    "    contingency_table = pd.crosstab(train[column], train['SalePrice'])\n",
    "    chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    results.append((column, chi2, p_value))\n",
    "\n",
    "# Criar um DataFrame com os resultados\n",
    "results_df = pd.DataFrame(results, columns=['Feature', 'Chi2', 'P-value'])\n",
    "\n",
    "# Ordenar os resultados pelo valor de p-value (do menor para o maior)\n",
    "results_df.sort_values(by='P-value', inplace=True)\n",
    "\n",
    "# Plotar o gráfico de barras dos valores de p-value\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='Feature', y='P-value')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('Teste Qui-quadrado: Valores de P-value por Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as variáveis para o modelo são as seguintes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_correlated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engenharia de Atributos\n",
    "Nessa seção vamos realizar insersão de valores, e codificação de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(['SalePrice'], axis=1).copy()\n",
    "Y = train['SalePrice'] # here we need to remove unnacessary columns if exist\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valores faltantes\n",
    "Vamos verificar as variáveis com valores faltantes. Para as variáveis com uma porcentagem menor que 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if is null mean greater than 0\n",
    "train_X.isnull().mean().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos deletar a variável _FireplaceQu_ e as demais, realizaremos um processo de inserção de dados. Para as variáveis contínuas, iremos realizar uma imputação simples pela mediana. Já para as variáveis discretas e categóricas iremos pela frequência. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_imputer = SimpleImputer(strategy='mean')\n",
    "discrete_imputer = SimpleImputer(strategy='most_frequent')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "continuous_imputer.fit(train_X[continuous])\n",
    "discrete_imputer.fit(train_X[discrete]) \n",
    "categorical_imputer.fit(train_X[categorical])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora o _SimpleImputer_ aprendeu a partir dos dados de treinamento, e realizará a imputação dos valores faltantes em todo o conjunto de dados de treinamento e teste. Nessa etapa não há vazamento de informações do teste para o treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[continuous] = continuous_imputer.transform(train_X[continuous])\n",
    "train_X[discrete] = discrete_imputer.transform(train_X[discrete])\n",
    "train_X[categorical] = categorical_imputer.transform(train_X[categorical])\n",
    "\n",
    "test_X[continuous] = continuous_imputer.transform(test_X[continuous])\n",
    "test_X[discrete] = discrete_imputer.transform(test_X[discrete])\n",
    "test_X[categorical] = categorical_imputer.transform(test_X[categorical])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que a insersção de dados foi feita corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X.isnull().mean().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.isnull().mean().sort_values(ascending=False).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não há valores faltantes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rare Labels\n",
    "É comum encontrarmos em grandes datasets alguns atrbutos que contêm categorias raras. Para aumentar o poder preditivo dos modelos e diminuir o número de categorias, emprega-se o **RareLabel Encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import encoding as ce\n",
    "rare_label_encoder = ce.RareLabelEncoder(\n",
    "    tol=0.05, # porcentagem mínima de observações para não ser considerado raro\n",
    "    n_categories=1,# numero de categorias para considerar raro\n",
    "    )\n",
    "rare_label_encoder.fit(train_X[categorical+discrete])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[categorical+discrete] = rare_label_encoder.transform(train_X[categorical+discrete])\n",
    "test_X[categorical+discrete] = rare_label_encoder.transform(test_X[categorical+discrete])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar o resultado, basta procurarmos a categoria \"Rare\" no conjunto de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#procurar por celulas com valore \"Rare\":\n",
    "train_X[categorical+discrete].apply(lambda x: x.str.contains('Rare').any(), axis=1).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversão de variáveis categóricas\n",
    "Existem diversas técnicas de conversão simbólica-numérica, dentre elas as mais comuns são: codificação binária, ordinal, por frequência, e pela mediana.\n",
    "\n",
    "Neste projeto, a opção escolhida foi a codificação ordinal, uma vez que muitas variáveis categóricas preservam algum certo de grau de progressão ou intensidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = ce.OrdinalEncoder(encoding_method='ordered',variables=categorical)\n",
    "ordinal_encoder.fit(train_X,train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = ordinal_encoder.transform(train_X)\n",
    "test_X = ordinal_encoder.transform(test_X)\n",
    "train_X[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Vamos verificar se há valores outliers nos atributos e vamos realizar o tratamento deles\n",
    "### Identificando Outlier com Z- Score\n",
    "O z-score nos da uma idéia do quanto um determinado ponto está afastado da média dos dados, isto é , ele mede quantos desvios padrão abaixo ou acima da média populacional ou amostral os dados estão:\n",
    "$$\n",
    "z=\\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "Onde:\n",
    "- x: observação\n",
    "- $\\mu$: média\n",
    "- $\\sigma$: desvio padrão\n",
    "Assumindo uma distribuição normal, sabe-se que 99,7% dos dados estão à uma distância de três desvios padrão da média. Com base nisso, será considerado nesse trabalho que dados com distância acima de três desvios padrão serão considerados outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filtrar apenas as colunas numéricas do DataFrame train após o drop\n",
    "# # numerical_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# # Criar uma cópia do DataFrame train\n",
    "# train_ZS = train[continuous]\n",
    "\n",
    "# # Calcular e atribuir o Z-score para cada coluna numérica, exceto a última\n",
    "# for col in numerical_features[:-1]:\n",
    "#     col_values = train_ZS[col].values\n",
    "#     zscore = stats.zscore(col_values)  # Calcula o Z-score para todos os valores da coluna\n",
    "#     outliers = (zscore > 3) | (zscore < -3)\n",
    "#     train_ZS.loc[outliers, col] = np.nan\n",
    "\n",
    "# # Criação do imputador KNN\n",
    "# imputer = KNNImputer(n_neighbors=15, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "# # Ajuste do imputador aos dados\n",
    "# imputer.fit(train_ZS)\n",
    "\n",
    "# # Imputação dos valores ausentes\n",
    "# train_ZS = pd.DataFrame(imputer.transform(train_ZS), columns=train_ZS.columns)\n",
    "\n",
    "# caminho_arquivo_excel = '../dataset/train_ZS.xlsx'\n",
    "# train_ZS.to_excel(caminho_arquivo_excel, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificando Outlier com Amplitude interquartil\n",
    "#### Percentil\n",
    "- percentil 25 : primeiro quaril\n",
    "- percentil 50 : segundo quartil ou mediana\n",
    "- percentil 75 : terceito quartil\n",
    "#### Amplitude interquartil\n",
    "É Diferença entre o terceiro quartil (Q3) e o primeiro quartil (Q1)\n",
    "Para identificar Outlier rom amplitude interquartil serão realizados os procedimentos abaixo:\n",
    "1. Ordenar os dados de forma crescente;\n",
    "2. Calcular o valor do primeiro e terceiro quartil\n",
    "3. Determinar a amplitude interquartil\n",
    "4. Encontrar o limite inferior =  Q1 - 1,5*amplitude interquartil\n",
    "5. Encontrar o limite superior =  Q3 + 1,5*amplitude interquartil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from feature_engine.outliers import OutlierTrimmer\n",
    "# # ver com Analise Exploratoria de Dados os dados que estão fora do padrão\n",
    "# trimmer = OutlierTrimmer(\n",
    "#     variables=continuous,\n",
    "#     capping_method=\"iqr\",\n",
    "#     tail=\"both\",\n",
    "#     fold=1.5,\n",
    "# )\n",
    "\n",
    "# trimmer.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar novo DataFrame com as colunas numéricas\n",
    "# Calcular o IQR e identificar outliers\n",
    "# for col in continuous:\n",
    "#     Q1 = train_X[col].quantile(0.25)\n",
    "#     Q3 = train_X[col].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "    \n",
    "#     limIn = Q1 - (IQR * 1.5)\n",
    "#     limSp = Q3 + (IQR * 1.5)\n",
    "    \n",
    "#     # Substituir outliers por np.nan\n",
    "#     # train_IQR.loc[(x[col] < limIn) | (train_IQR[col] > limSp), col] = np.nan\n",
    "#     inliers = train_X[col].between(limIn, limSp)\n",
    "#     train_X = train_X.loc[inliers]\n",
    "\n",
    "#     inliers = test_X[col].between(limIn, limSp)\n",
    "#     test_X = test_X.loc[inliers]\n",
    "\n",
    "# Criar imputador KNN\n",
    "# imputer = KNNImputer(n_neighbors=15, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "# Ajustar imputador aos dados\n",
    "# imputer.fit(train_IQR)\n",
    "\n",
    "# Imputar valores ausentes\n",
    "# train_IQR = pd.DataFrame(imputer.transform(train_IQR), columns=train_IQR.columns)\n",
    "\n",
    "# caminho_arquivo_excel = '../dataset/train_IQR.xlsx'\n",
    "# train_IQR.to_excel(caminho_arquivo_excel, index=False)\n",
    "\n",
    "# caminho_arquivo_excel = '../dataset/train_2.xlsx'\n",
    "# train[numerical_features].to_excel(caminho_arquivo_excel, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputando valores aos outliers\n",
    "Em ambos os métodos acima, foram identificados os outliers e para cada um deles foi atribuido um valor ausente, ou seja, eles foram excluidos dos dados.\n",
    "Para preencher os dados ausentes foi utilizada uma técnica de imputação de valores com base em proximidade, denominada KNN. O imputador KNN (K-Nearest Neighbors) é uma técnica dque pode ser utilizada para preencher valores ausentes em conjuntos de dados. Ele é um algoritmo de aprendizado de máquina capaz de prever valores ausentes com base na similaridade entre as amostras do conjunto de dados.\n",
    "\n",
    "Inicialmente ele encontra os K vizinhos mais próximos (específicamente 15 nesse trabalho) para cada valor ausente, calculando a distância entre a amostra com valor ausente e todas as outras amostras no conjunto de dados. Ele seleciona as K amostras mais próximas com base em uma métrica de distância que, neste caso, é a distância euclidiana.\n",
    "\n",
    "Em seguida, o KNN calcula um valor imputado para o valor ausente com base na média arimética dos valores dos vizinhos (vizinhos com pesos iguais). Por fim, o valor imputado é atribuído a cada valor ausente no conjunto de dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção de Atributos (opcional)\n",
    "Nessa seção vamos realizar a seleção de atributos relevantes para o modelo.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino e Teste"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção vamos treinar um modelo de regressão que gere uma predição dos valores de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressao linear\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#arvore de deciao\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# SVM\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Adaline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#MLP\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "#cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos o dataset de treino na proporção de 70-30 em um dataset de treino e outro de teste. Perceba que o tamanho do dataset de teste realmente é 30% do dataset original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_X.shape, train_Y.shape)\n",
    "print(test_X.shape, len(test_Y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro modelo será uma regressão linear utilizando a biblioteca do scikit-learn de linear_model importando a classe LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LR = LinearRegression()\n",
    "LR.fit(train_X, train_Y)\n",
    "\n",
    "LR_predicted = LR.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvore de Decisão"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O segundo modelo será um regressor de Árvore de Decisão utilizando a biblioteca do scikit-learn de tree importando a classe DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTR = DecisionTreeRegressor(criterion='squared_error', max_depth=15, min_samples_split=5, min_samples_leaf=5)\n",
    "DTR.fit(train_X, train_Y)\n",
    "\n",
    "DTR_predicted = DTR.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O terceiro modelo será um SVM utilizando a biblioteca do scikit-learn de svm importando a classe SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVM = make_pipeline(RobustScaler(), SVR(kernel='linear', C=1000))\n",
    "SVM.fit(train_X, train_Y)\n",
    "\n",
    "SVM_predicted = SVM.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaline - Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ADALINE = SGDRegressor(loss='huber', learning_rate='constant', eta0=0.01, max_iter=1000)\n",
    "ADALINE.fit(train_X, train_Y)\n",
    "\n",
    "ADALINE_predicted = ADALINE.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP - Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=10000, random_state=42)\n",
    "\n",
    "MLP.fit(train_X, train_Y)\n",
    "MLP_predicted = MLP.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NB = GaussianNB(var_smoothing=1e-9)\n",
    "\n",
    "NB.fit(train_X, train_Y)\n",
    "NB_predicted = NB.predict(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção vamos avaliar a performance do modelo gerado.\n",
    "\n",
    "Os principais critério que vamos avaliar vão ser os seguintes:\n",
    "- $R^2$:\n",
    "O primeiro é o coeficiente de determinação, usualmente expresso por $R^2$. O coeficiente de determinação é a razão da variância do alvo, explicado ou predito pelo modelo, pela a variância total do alvo. É um valor com limites entre $0$ e $1$, e quanto mais próximo de 1 maior a capacidade do modelo em explicar ou prever a variância da variável alvo.\n",
    "$$R^2 = \\frac{\\sum(\\hat{y} - \\bar{y})^2}{\\sum(y - \\bar{y})^2}$$\n",
    "\n",
    "- $RMSE$:\n",
    "A segunda métrica é o erro médio quadrático (_Mean Squared Error_). O $MSE$ é a média das diferenças entre o valor alvo predito e o valor real ao quadrado. Nesse sentido, é sempre maior que zero, e quanto menor o valor de $MSE$ maior a acurácia das predições do modelo. Nesse projeto, iremos utilizar a raiz quadrada de $MSE$, chamada de $RMSE$.\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}(y_i-p_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# avaliando\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from functions.actual_vs_pred_plot import actual_vs_pred_plot\n",
    "from functions.model_residual_plot import model_residual_plot\n",
    "from functions.regression_metrics import regression_metrics\n",
    "from functions.model_dist_plot import model_dist_plot\n",
    "import seaborn as sns\n",
    "\n",
    "list_models_name = [\n",
    "    \"Regressão Linear\",\n",
    "    \"Árvore de Decisão\",\n",
    "    \"SVM\",\n",
    "    \"ADALINE\",\n",
    "    \"MLP\",\n",
    "    \"Naive Bayes\",\n",
    "]\n",
    "list_models = [LR, DTR, SVM, ADALINE, MLP, NB]\n",
    "list_models_predicted = [LR_predicted, DTR_predicted, SVM_predicted, ADALINE_predicted, MLP_predicted, NB_predicted]\n",
    "list_models_RMSE = []\n",
    "list_models_R2 = []\n",
    "list_models_MAPE = []\n",
    "\n",
    "for i in range(0, len(list_models)):\n",
    "    model_predicted = list_models_predicted[i]\n",
    "    model_name = list_models_name[i]\n",
    "    print(\"\\nModelo \" + model_name + \":\")\n",
    "    rmse, r2, mape = regression_metrics(model_predicted, test_Y,model_name)\n",
    "    list_models_RMSE.append(rmse)\n",
    "    list_models_R2.append(r2)\n",
    "    list_models_MAPE.append(mape)\n",
    "\n",
    "    model_dist_plot(test_Y, model_predicted, model_name)\n",
    "    \n",
    "    actual_vs_pred_plot(test_Y, model_predicted,model_name)\n",
    "\n",
    "    # Create residual plot\n",
    "    model_residual_plot(test_Y, model_predicted, model_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando os valores de $MAPE$ (erro percentual absoluto médio) de cada modelo gerado, obtemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# comparando os modelos por MAPE:\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.barplot(x=list_models_name, y=list_models_MAPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível perceber que SVM é o modelo que possui a menor média dos erros percentuais absolutos entre os valores preditos e os valores verdadeiros, enquanto que os modelos de Redes Neurais possuíram altas médias."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando os $RMSE$ de cada modelo gerado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comparando os modelos por RMSE:\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.barplot(x=list_models_name, y=list_models_RMSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma, é possível perceber que Árvore de Decisão é o modelo com o menor RMSE, enquanto que novamente os modelos de redes neurais Adaline e MLP tiveram valores altos de RMSE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando os scores de $R2$ de cada modelo gerado, obtemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comparando os modelos por R2:\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.barplot(x=list_models_name, y=list_models_R2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, Árvore de Decisão se destaca como o melhor modelo nesse quesito, onde quanto mais próximo de 1 é o score de R_2, melhor é o ajuste do modelo aos dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ADALINE$ e $MLP$ são ambos modelos de redes neurais, sendo que a $MLP$ é mais complexa do que o $ADALINE$. Redes neurais exigem um grande volume de dados para aprender adequadamente, e podem ter desempenho abaixo do esperado se houver poucos dados de treinamento. Além disso, esses modelos podem sofrer de overfitting se não forem adequadamente configurados. Isso poderia explicar os resultados elevados de MAPE e RMSE (indicativos de erro) e baixos valores de R2 (indicativo de acurácia)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por outro lado, o $SVM$, a $Regressão$ $Linear$ e a $Árvore$ $de$ $Decisão$ são algoritmos mais simples e menos propensos ao overfitting. O $SVM$ é especialmente conhecido por sua habilidade em lidar com espaços de alta dimensionalidade e encontrar fronteiras de decisão complexas, o que pode explicar por que ele tem o menor MAPE. No entanto, isso não significa necessariamente que ele terá o menor RMSE ou o maior R2, já que essas métricas avaliam aspectos ligeiramente diferentes do modelo. O fato de a $Árvore$ $de$ $Decisão$ ter o menor RMSE e o maior R2 pode indicar que ela está fazendo um bom trabalho de generalização."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O $Naive$ $Bayes$ faz uma suposição de independência entre os atributos que raramente é verdadeira na prática, o que pode limitar sua acurácia. Se os atributos em seus dados são altamente correlacionados (como muitas vezes acontece em dados de imóveis), isso poderia explicar por que o $Naive$ $Bayes$ não tem desempenho tão bom quanto os outros modelos, mas ainda assim é melhor do que o $ADALINE$ e o $MLP$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, seguindo a métrica de _Root Mean Squared Error_ como a principal para fazer seleção dos modelos, o melhor modelo foi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_index = np.argmin(list_models_RMSE)\n",
    "model_name = list_models_name[model_index]\n",
    "model_predicted = list_models_predicted[model_index]\n",
    "print(\"\\nModelo com melhor R2: \" + model_name)\n",
    "print(\"MAPE: \" + str(list_models_MAPE[model_index]))\n",
    "print(\"RMSE: \" + str(list_models_RMSE[model_index]))\n",
    "print(\"R2: \" + str(list_models_R2[model_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# melhor_modelo = list_models[model_index]\n",
    "\n",
    "# validate_columns = features_correlated.drop(['SalePrice'])\n",
    "# validate_df = test[validate_columns]\n",
    "\n",
    "# # Eliminate these lines after - Preprocess the data to handle missing values\n",
    "# # imputer = SimpleImputer(strategy='mean')\n",
    "# # validate_df = pd.DataFrame(imputer.fit_transform(validate_df), columns=validate_columns)\n",
    "\n",
    "# validate = validate_df.copy()\n",
    "# validate['SalePrice'] = melhor_modelo.predict(validate_df)\n",
    "\n",
    "# # validate\n",
    "\n",
    "# predicted_sale_price = validate\n",
    "# predicted_sale_price.to_csv(\"SalePrice_predicted.csv\", index=False)\n",
    "# predicted_sale_price.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando Pipelines e Validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerações finais"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
